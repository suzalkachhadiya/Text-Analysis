itle netclan
source url httpsinsightsblackcoffercomimmigrationdatawarehouseaibasedrecommendations


home
our success stories
immigration datawarehouse  aibased recommendations
client background
client
a leading business school worldwide
industry type
rd
services
rd innovation
organization size

project objective
objective of this project is to research and collect news article data sourcing from canada based on the keyword
project description
there were  phases of the project
phase 
 data collection and selection
data related to anyone coming to canada new comers
data related to anyone coming to canada new comers
canadian policy to new comers
ie from any country to canada
data containing news press think tanks government policy documents or research institutions releasing the news or press about
the news source should be limited to canada only
time span  to 
output excel having urls or the documents along with the source type keywords and date on which that article is posted
phase 
 documents text data extraction
develop tool to collect and extract data from each url
clean and save the texts in the text documents
phase 
 textual analysis
sentiment analysis
analysis of readability
topic modelling
our solution
we provide them with completed phase  in an excel sheet and ongoing samples for phase  also work for phase  has been started in between to complete the project as soon as possible in a best way
project deliverables
there is a file containing excel sheet and a word file containing a summary of the dataset and folders of text files containing samples of data from phase 
tools used
python pycharm jupyter notebook microsoft excel google chrome is used to complete different phases of this project
languagetechniques used
python programming language is used to do web scraping automation data engineering in this project
models used
sdlc is a process followed for a software project within a software organization it consists of a detailed plan describing how to develop maintain replace and alter or enhance specific software the life cycle defines a methodology for improving the quality of software and the overall development process
we are using iterative waterfall sdlc model as we have to follow our development of software in phases and we also need feedback on every step of the development of our project so as to keep track of the occurring changes with every step
figure  sdlc iterative waterfall model
skills used
data scraping cleaning preprocessing and creating data pipelines are used in this project
databases used
we used the traditional way of storing the data ie file systems
what are the technical challenges faced during project execution
there were a lot of challenges we faced during the project execution
as on the internet raw data is available to us so to search for the important data specifically related to canada only with a lot of keywords was a challenging part for us
then if we somehow manage to do the task by automating it upto some extent only we are required to find the dates of the articles news think tanks documents etc that was also a challenging part
while working on phase  we need to scrape the data from the urls so sometimes the news articles were removed from the website which we earlier took in our datasets which cause problems in extracting the data
then cleaning the webpages was also challenge for us because this project is for research so data is important to us so it was difficult to take only that data from website which we require and are most important
how the technical challenges were solved
below are the points used to solve the above technical challenges
we used sitemaps of websites to find different articles that we require according to the keywords manual research was done to find out which url will solve the purpose manual checking of results of automation tools that we created was done
to find the dates of the articles we wrote multiple regular expressions that will find the match for the dates that we need also manual checking was done after that
to scrape removed webpages we used wayback machine or google archives which stores all the deleted webpages
to clean the data we filtered out various html tags classes ids by using regex manual research
project snapshots
related articles
more from author
integrating machine learning code into kubeflow pipeline  kuberflow mlops kubernetes
facial recognition attendance system
face recognition using deepface
most popular insights
monetization of data  innovate to harvest the full value of
june  
ikiga data a global careers data and insights platform
march  
how telehealth and telemedicine helping people to fight against covid
april  
how prepared is india to tackle a possible covid outbreak
april  
load more
recommended insights
sports prediction model for multiple sports leagues
ai and ml technologies to evaluate learning assessments
ai ml and iot driven entry management and monitoring
deploy nodejs apps to google app engine google cloud platform