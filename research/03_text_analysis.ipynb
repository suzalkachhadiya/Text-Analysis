{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\DataScience\\\\Projects\\\\Text_analysis\\\\research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\DataScience\\\\Projects\\\\Text_analysis'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TextAnalysisConfig:\n",
    "    root_dir: Path\n",
    "    positive_words_path: Path\n",
    "    negative_words_path: Path\n",
    "    merged_stop_words_path: Path\n",
    "    raw_text_path: Path\n",
    "    processed_text_path: Path\n",
    "    output_file_path: Path\n",
    "    destination_folder: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.text_analysis.constants import *\n",
    "from src.text_analysis.utils.common import read_yaml_file, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml_file(config_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    def get_text_analysis_config(self) -> TextAnalysisConfig:\n",
    "        config = self.config.text_analysis\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        text_analysis_config = TextAnalysisConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            processed_text_path=config.processed_text_path,\n",
    "            positive_words_path=config.positive_words_path,\n",
    "            negative_words_path=config.positive_words_path,\n",
    "            merged_stop_words_path=config.merged_stop_words_path,\n",
    "            raw_text_path=config.raw_text_path,\n",
    "            output_file_path=config.output_file_path,\n",
    "            destination_folder=config.destination_folder,\n",
    "        )\n",
    "\n",
    "        return text_analysis_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextAnalysis:\n",
    "    def __init__(self, config: TextAnalysisConfig):\n",
    "        self.config = config\n",
    "        \n",
    "    def word_count(self,text):\n",
    "        \n",
    "        stop_words=set(open(self.config.merged_stop_words_path,encoding=\"ISO-8859-1\").read().split())\n",
    "        \n",
    "        words=word_tokenize(text)\n",
    "        words=[word for word in words if word not in stop_words]\n",
    "        \n",
    "        return len(words), words\n",
    "    \n",
    "    def total_words_count(self,text):\n",
    "        total_words=0\n",
    "        avg_word_len=0\n",
    "        for sentence in text.split(\"\\n\"):\n",
    "            for word in sentence.split(\" \"):\n",
    "                total_words=total_words+1\n",
    "                avg_word_len=avg_word_len+len(word)\n",
    "                \n",
    "        return total_words, avg_word_len\n",
    "    \n",
    "    def sentiment_check(self,text):\n",
    "        \n",
    "        positive_words_path=self.config.positive_words_path\n",
    "        negative_words_path=self.config.negative_words_path\n",
    "        \n",
    "        positive_words = set(open(positive_words_path,encoding=\"ISO-8859-1\").read().split())\n",
    "        negative_words = set(open(negative_words_path,encoding=\"ISO-8859-1\").read().split())\n",
    "        \n",
    "        positive_score = 0\n",
    "        negative_score = 0\n",
    "        total_words=0\n",
    "        \n",
    "        for sentence in text.split(\"\\n\"):\n",
    "            for word in sentence.split(\" \"):\n",
    "                total_words=total_words+1\n",
    "                if word in positive_words:\n",
    "                    positive_score += 1\n",
    "                elif word in negative_words:\n",
    "                    negative_score -= 1\n",
    "                \n",
    "        polarity_score = (positive_score + negative_score) / ((positive_score - negative_score) + 0.000001)\n",
    "        subjectivity_score =(positive_score + negative_score)/ (total_words + 0.000001)\n",
    "        \n",
    "        return positive_score, negative_score, polarity_score, subjectivity_score\n",
    "    \n",
    "    def sentence_analysis(self,text,total_words):\n",
    "        avg_sen_len=total_words/len(text.split(\"\\n\"))\n",
    "\n",
    "        avg_words_sen=0\n",
    "        num_sen=0\n",
    "        for sentence in text.split(\"\\n\"):\n",
    "            avg_words_sen = avg_words_sen + len(sentence.split(\" \"))\n",
    "            num_sen=num_sen+1\n",
    "\n",
    "        avg_words_sen=avg_words_sen / num_sen\n",
    "        return avg_words_sen, avg_sen_len\n",
    "    \n",
    "    def count_syllables(self,word):\n",
    "        word = word.lower()\n",
    "        vowels = \"aeiou\"\n",
    "        syllable_count = 0\n",
    "        previous_char_was_vowel = False\n",
    "\n",
    "        for char in word:\n",
    "            if char in vowels:\n",
    "                # Count only when encountering a vowel for the first time in a sequence\n",
    "                if not previous_char_was_vowel:\n",
    "                    syllable_count += 1\n",
    "                previous_char_was_vowel = True\n",
    "            else:\n",
    "                previous_char_was_vowel = False\n",
    "\n",
    "        # Adjustments for silent 'e' at the end\n",
    "        if word.endswith(\"e\") or word.endswith(\"es\") or word.endswith(\"ed\"):\n",
    "            syllable_count -= 1\n",
    "        return max(1, syllable_count)\n",
    "    \n",
    "    # Function to count complex words in text\n",
    "    def count_complex_words(self,text,avg_sen_len):\n",
    "        complex_words_count=0\n",
    "        cnt=0\n",
    "        for sentence in text.split(\"\\n\"):\n",
    "            # Count complex words based on syllable count\n",
    "            for word in sentence.split(\" \"):\n",
    "                cnt=cnt+1\n",
    "                if self.count_syllables(word) >= 3:\n",
    "                    complex_words_count += 1\n",
    "        \n",
    "        pct_complex_words=complex_words_count/cnt\n",
    "        fog_index = 0.4 * (avg_sen_len + pct_complex_words)\n",
    "\n",
    "        \n",
    "        return complex_words_count, pct_complex_words, fog_index\n",
    "    \n",
    "    def syllable_count_per_word(self,text,total_words):\n",
    "        syllable_words_count=0\n",
    "        for sentence in text.split(\"\\n\"):\n",
    "            for word in sentence.split(\" \"):\n",
    "                syllable_words_count=syllable_words_count+self.count_syllables(word)\n",
    "\n",
    "        syllable_words_count=syllable_words_count/total_words\n",
    "        return syllable_words_count\n",
    "    \n",
    "    def pronoun_count(self,text):\n",
    "        # Pronouns in lowercase\n",
    "        pronouns = [\n",
    "            # Personal pronouns (subjective and objective)\n",
    "            \"i\", \"me\", \"you\", \"he\", \"him\", \"she\", \"her\", \"it\", \"we\", \"us\", \"they\", \"them\",\n",
    "\n",
    "            # Possessive pronouns\n",
    "            \"my\", \"mine\", \"your\", \"yours\", \"his\", \"her\", \"hers\", \"its\", \"our\", \"ours\", \"their\", \"theirs\",\n",
    "\n",
    "            # Reflexive pronouns\n",
    "            \"myself\", \"yourself\", \"himself\", \"herself\", \"itself\", \"ourselves\", \"yourselves\", \"themselves\",\n",
    "\n",
    "            # Demonstrative pronouns\n",
    "            \"this\", \"that\", \"these\", \"those\",\n",
    "\n",
    "            # Relative pronouns\n",
    "            \"who\", \"whom\", \"whose\", \"which\", \"that\",\n",
    "\n",
    "            # Interrogative pronouns\n",
    "            \"who\", \"whom\", \"whose\", \"which\", \"what\",\n",
    "\n",
    "            # Indefinite pronouns\n",
    "            \"anybody\", \"anyone\", \"anything\", \"each\", \"everybody\", \"everyone\", \"everything\",\n",
    "            \"nobody\", \"no one\", \"nothing\", \"somebody\", \"someone\", \"something\",\"no one\", \"nothing\", \"somebody\", \"someone\", \"something\",\n",
    "            \"any\", \"all\", \"some\", \"none\", \"both\", \"few\", \"several\", \"many\", \"others\"\n",
    "        ]\n",
    "\n",
    "        num_pronoun=0\n",
    "        for sentence in text.split(\"\\n\"):\n",
    "            for word in sentence.split(\" \"):\n",
    "                if word.lower() in pronouns:\n",
    "                    num_pronoun=num_pronoun+1\n",
    "\n",
    "        return num_pronoun\n",
    "\n",
    "    def text_analysis(self):\n",
    "        processed_text_path = self.config.processed_text_path\n",
    "        raw_text_path = self.config.raw_text_path\n",
    "        \n",
    "        # processed_text_path_list=os.listdir(processed_text_path),\n",
    "        # raw_text_path_list=os.listdir(raw_text_path),\n",
    "        \n",
    "        url_id_list=[]\n",
    "        url_list=[]\n",
    "        positive_score_list=[]\n",
    "        negative_score_list=[]\n",
    "        polarity_score_list=[]\n",
    "        subjectivity_score_list=[]\n",
    "        avg_sen_len_list=[]\n",
    "        pct_complex_words_list=[]\n",
    "        fog_index_list=[]\n",
    "        avg_words_sen_list=[]\n",
    "        complex_words_count_list=[]\n",
    "        word_count_list=[]\n",
    "        syllable_words_count_list=[]\n",
    "        num_pronoun_list=[]\n",
    "        avg_word_len_list=[]\n",
    "        \n",
    "        for processed_filename, raw_filename in zip(os.listdir(processed_text_path),os.listdir(raw_text_path)):\n",
    "            raw_file_path=os.path.join(raw_text_path,raw_filename)\n",
    "            processed_file_path=os.path.join(processed_text_path,processed_filename)\n",
    "            print(processed_file_path)\n",
    "            with open(processed_file_path, 'r', encoding='utf-8') as file:\n",
    "                text = file.read()\n",
    "                \n",
    "            word_count, words = self.word_count(text)\n",
    "            word_count_list.append(word_count)\n",
    "            \n",
    "            total_words, avg_word_len=self.total_words_count(text)\n",
    "            avg_word_len_list.append(avg_word_len)\n",
    "            \n",
    "            positive_score, negative_score, polarity_score, subjectivity_score = self.sentiment_check(text)\n",
    "            positive_score_list.append(positive_score)\n",
    "            negative_score_list.append(negative_score)\n",
    "            polarity_score_list.append(polarity_score)\n",
    "            subjectivity_score_list.append(subjectivity_score)\n",
    "            \n",
    "            avg_words_sen, avg_sen_len=self.sentence_analysis(text,total_words)\n",
    "            avg_sen_len_list.append(avg_sen_len)\n",
    "            avg_words_sen_list.append(avg_sen_len)\n",
    "            \n",
    "            complex_words_count, pct_complex_words, fog_index= self.count_complex_words(text,avg_sen_len)\n",
    "            complex_words_count_list.append(complex_words_count)\n",
    "            pct_complex_words_list.append(pct_complex_words)\n",
    "            fog_index_list.append(fog_index)\n",
    "            \n",
    "            syllable_words_count=self.syllable_count_per_word(text,total_words)\n",
    "            syllable_words_count_list.append(syllable_words_count)\n",
    "            \n",
    "            pronouns_count=self.pronoun_count(text)\n",
    "            num_pronoun_list.append(pronouns_count)\n",
    "            \n",
    "            # Initialize variables for URL_ID and URL\n",
    "            url_id = None\n",
    "            url = None\n",
    "\n",
    "            # Open and read the file\n",
    "            with open(raw_file_path, \"r\", encoding=\"ISO-8859-1\") as file:\n",
    "                for line in file:\n",
    "                    # Check if the line contains the URL_ID by matching \"Title: \"\n",
    "                    if line.startswith(\"Title:\"):\n",
    "                        url_id = line.split(\"Title:\")[1].strip()\n",
    "\n",
    "                    # Check if the line contains the URL by matching \"Source URL: \"\n",
    "                    elif line.startswith(\"Source URL:\"):\n",
    "                        url = line.split(\"Source URL:\")[1].strip()\n",
    "                        \n",
    "            url_id_list.append(url_id)\n",
    "            url_list.append(url)\n",
    "                        \n",
    "        data = {\n",
    "                \"URL_ID\": url_id_list,\n",
    "                \"URL\": url_list,\n",
    "                \"POSITIVE SCORE\": positive_score_list,\n",
    "                \"NEGATIVE SCORE\": negative_score_list,\n",
    "                \"POLARITY SCORE\": polarity_score_list,\n",
    "                \"SUBJECTIVITY SCORE\": subjectivity_score_list,\n",
    "                \"AVG SENTENCE LENGTH\": avg_sen_len_list,\n",
    "                \"PERCENTAGE OF COMPLEX WORDS\": pct_complex_words_list,\n",
    "                \"FOG INDEX\": fog_index_list,\n",
    "                \"AVG NUMBER OF WORDS PER SENTENCE\": avg_words_sen_list,\n",
    "                \"COMPLEX WORD COUNT\": complex_words_count_list,\n",
    "                \"WORD COUNT\": word_count_list,\n",
    "                \"SYLLABLE PER WORD\": syllable_words_count_list,\n",
    "                \"PERSONAL PRONOUNS\": num_pronoun_list,\n",
    "                \"AVG WORD LENGTH\": avg_word_len_list\n",
    "            }\n",
    "        \n",
    "        print(len(url_id_list))\n",
    "        print(len(url_list))\n",
    "        print(len(positive_score_list))\n",
    "        print(len(negative_score_list))\n",
    "        print(len(polarity_score_list))\n",
    "        print(len(subjectivity_score_list))\n",
    "        print(len(avg_sen_len_list))\n",
    "        print(len(pct_complex_words_list))\n",
    "        print(len(fog_index_list))\n",
    "        print(len(avg_words_sen_list))\n",
    "        print(len(complex_words_count_list))\n",
    "        print(len(word_count_list))\n",
    "        print(len(syllable_words_count_list))\n",
    "        print(len(num_pronoun_list))\n",
    "        print(len(avg_word_len_list))\n",
    "        \n",
    "        # Create DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Specify the file path to save the Excel file\n",
    "        file_path = self.config.output_file_path\n",
    "\n",
    "        # Write to Excel file\n",
    "        df.to_excel(file_path, index=False)\n",
    "\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-14 19:06:17,784: INFO:common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2024-11-14 19:06:17,787: INFO:common: createD Directory at:artifacts]\n",
      "[2024-11-14 19:06:17,787: INFO:common: createD Directory at:artifacts/text_analysis]\n",
      "artifacts/text_processing\\Netclan20241017.txt\n",
      "artifacts/text_processing\\Netclan20241018.txt\n",
      "artifacts/text_processing\\Netclan20241019.txt\n",
      "artifacts/text_processing\\Netclan20241020.txt\n",
      "artifacts/text_processing\\Netclan20241021.txt\n",
      "artifacts/text_processing\\Netclan20241022.txt\n",
      "artifacts/text_processing\\Netclan20241023.txt\n",
      "artifacts/text_processing\\Netclan20241024.txt\n",
      "artifacts/text_processing\\Netclan20241025.txt\n",
      "artifacts/text_processing\\Netclan20241026.txt\n",
      "artifacts/text_processing\\Netclan20241027.txt\n",
      "artifacts/text_processing\\Netclan20241028.txt\n",
      "artifacts/text_processing\\Netclan20241029.txt\n",
      "artifacts/text_processing\\Netclan20241030.txt\n",
      "artifacts/text_processing\\Netclan20241031.txt\n",
      "artifacts/text_processing\\Netclan20241032.txt\n",
      "artifacts/text_processing\\Netclan20241033.txt\n",
      "artifacts/text_processing\\Netclan20241034.txt\n",
      "artifacts/text_processing\\Netclan20241035.txt\n",
      "artifacts/text_processing\\Netclan20241036.txt\n",
      "artifacts/text_processing\\Netclan20241037.txt\n",
      "artifacts/text_processing\\Netclan20241038.txt\n",
      "artifacts/text_processing\\Netclan20241039.txt\n",
      "artifacts/text_processing\\Netclan20241040.txt\n",
      "artifacts/text_processing\\Netclan20241041.txt\n",
      "artifacts/text_processing\\Netclan20241042.txt\n",
      "artifacts/text_processing\\Netclan20241043.txt\n",
      "artifacts/text_processing\\Netclan20241044.txt\n",
      "artifacts/text_processing\\Netclan20241045.txt\n",
      "artifacts/text_processing\\Netclan20241046.txt\n",
      "artifacts/text_processing\\Netclan20241047.txt\n",
      "artifacts/text_processing\\Netclan20241048.txt\n",
      "artifacts/text_processing\\Netclan20241049.txt\n",
      "artifacts/text_processing\\Netclan20241050.txt\n",
      "artifacts/text_processing\\Netclan20241051.txt\n",
      "artifacts/text_processing\\Netclan20241052.txt\n",
      "artifacts/text_processing\\Netclan20241053.txt\n",
      "artifacts/text_processing\\Netclan20241054.txt\n",
      "artifacts/text_processing\\Netclan20241055.txt\n",
      "artifacts/text_processing\\Netclan20241056.txt\n",
      "artifacts/text_processing\\Netclan20241057.txt\n",
      "artifacts/text_processing\\Netclan20241058.txt\n",
      "artifacts/text_processing\\Netclan20241059.txt\n",
      "artifacts/text_processing\\Netclan20241060.txt\n",
      "artifacts/text_processing\\Netclan20241061.txt\n",
      "artifacts/text_processing\\Netclan20241062.txt\n",
      "artifacts/text_processing\\Netclan20241063.txt\n",
      "artifacts/text_processing\\Netclan20241064.txt\n",
      "artifacts/text_processing\\Netclan20241065.txt\n",
      "artifacts/text_processing\\Netclan20241066.txt\n",
      "artifacts/text_processing\\Netclan20241067.txt\n",
      "artifacts/text_processing\\Netclan20241068.txt\n",
      "artifacts/text_processing\\Netclan20241069.txt\n",
      "artifacts/text_processing\\Netclan20241070.txt\n",
      "artifacts/text_processing\\Netclan20241071.txt\n",
      "artifacts/text_processing\\Netclan20241072.txt\n",
      "artifacts/text_processing\\Netclan20241073.txt\n",
      "artifacts/text_processing\\Netclan20241074.txt\n",
      "artifacts/text_processing\\Netclan20241075.txt\n",
      "artifacts/text_processing\\Netclan20241076.txt\n",
      "artifacts/text_processing\\Netclan20241077.txt\n",
      "artifacts/text_processing\\Netclan20241078.txt\n",
      "artifacts/text_processing\\Netclan20241079.txt\n",
      "artifacts/text_processing\\Netclan20241080.txt\n",
      "artifacts/text_processing\\Netclan20241081.txt\n",
      "artifacts/text_processing\\Netclan20241082.txt\n",
      "artifacts/text_processing\\Netclan20241083.txt\n",
      "artifacts/text_processing\\Netclan20241084.txt\n",
      "artifacts/text_processing\\Netclan20241085.txt\n",
      "artifacts/text_processing\\Netclan20241086.txt\n",
      "artifacts/text_processing\\Netclan20241087.txt\n",
      "artifacts/text_processing\\Netclan20241088.txt\n",
      "artifacts/text_processing\\Netclan20241089.txt\n",
      "artifacts/text_processing\\Netclan20241090.txt\n",
      "artifacts/text_processing\\Netclan20241091.txt\n",
      "artifacts/text_processing\\Netclan20241092.txt\n",
      "artifacts/text_processing\\Netclan20241093.txt\n",
      "artifacts/text_processing\\Netclan20241094.txt\n",
      "artifacts/text_processing\\Netclan20241095.txt\n",
      "artifacts/text_processing\\Netclan20241096.txt\n",
      "artifacts/text_processing\\Netclan20241097.txt\n",
      "artifacts/text_processing\\Netclan20241098.txt\n",
      "artifacts/text_processing\\Netclan20241099.txt\n",
      "artifacts/text_processing\\Netclan20241100.txt\n",
      "artifacts/text_processing\\Netclan20241101.txt\n",
      "artifacts/text_processing\\Netclan20241102.txt\n",
      "artifacts/text_processing\\Netclan20241103.txt\n",
      "artifacts/text_processing\\Netclan20241104.txt\n",
      "artifacts/text_processing\\Netclan20241105.txt\n",
      "artifacts/text_processing\\Netclan20241106.txt\n",
      "artifacts/text_processing\\Netclan20241107.txt\n",
      "artifacts/text_processing\\Netclan20241108.txt\n",
      "artifacts/text_processing\\Netclan20241109.txt\n",
      "artifacts/text_processing\\Netclan20241110.txt\n",
      "artifacts/text_processing\\Netclan20241111.txt\n",
      "artifacts/text_processing\\Netclan20241112.txt\n",
      "artifacts/text_processing\\Netclan20241113.txt\n",
      "artifacts/text_processing\\Netclan20241114.txt\n",
      "artifacts/text_processing\\Netclan20241115.txt\n",
      "artifacts/text_processing\\Netclan20241116.txt\n",
      "artifacts/text_processing\\Netclan20241117.txt\n",
      "artifacts/text_processing\\Netclan20241118.txt\n",
      "artifacts/text_processing\\Netclan20241119.txt\n",
      "artifacts/text_processing\\Netclan20241120.txt\n",
      "artifacts/text_processing\\Netclan20241121.txt\n",
      "artifacts/text_processing\\Netclan20241122.txt\n",
      "artifacts/text_processing\\Netclan20241123.txt\n",
      "artifacts/text_processing\\Netclan20241124.txt\n",
      "artifacts/text_processing\\Netclan20241125.txt\n",
      "artifacts/text_processing\\Netclan20241126.txt\n",
      "artifacts/text_processing\\Netclan20241127.txt\n",
      "artifacts/text_processing\\Netclan20241128.txt\n",
      "artifacts/text_processing\\Netclan20241129.txt\n",
      "artifacts/text_processing\\Netclan20241130.txt\n",
      "artifacts/text_processing\\Netclan20241131.txt\n",
      "artifacts/text_processing\\Netclan20241132.txt\n",
      "artifacts/text_processing\\Netclan20241133.txt\n",
      "artifacts/text_processing\\Netclan20241134.txt\n",
      "artifacts/text_processing\\Netclan20241135.txt\n",
      "artifacts/text_processing\\Netclan20241136.txt\n",
      "artifacts/text_processing\\Netclan20241137.txt\n",
      "artifacts/text_processing\\Netclan20241138.txt\n",
      "artifacts/text_processing\\Netclan20241139.txt\n",
      "artifacts/text_processing\\Netclan20241140.txt\n",
      "artifacts/text_processing\\Netclan20241141.txt\n",
      "artifacts/text_processing\\Netclan20241142.txt\n",
      "artifacts/text_processing\\Netclan20241143.txt\n",
      "artifacts/text_processing\\Netclan20241144.txt\n",
      "artifacts/text_processing\\Netclan20241145.txt\n",
      "artifacts/text_processing\\Netclan20241146.txt\n",
      "artifacts/text_processing\\Netclan20241147.txt\n",
      "artifacts/text_processing\\Netclan20241148.txt\n",
      "artifacts/text_processing\\Netclan20241149.txt\n",
      "artifacts/text_processing\\Netclan20241150.txt\n",
      "artifacts/text_processing\\Netclan20241151.txt\n",
      "artifacts/text_processing\\Netclan20241152.txt\n",
      "artifacts/text_processing\\Netclan20241153.txt\n",
      "artifacts/text_processing\\Netclan20241154.txt\n",
      "artifacts/text_processing\\Netclan20241155.txt\n",
      "artifacts/text_processing\\Netclan20241156.txt\n",
      "artifacts/text_processing\\Netclan20241157.txt\n",
      "artifacts/text_processing\\Netclan20241158.txt\n",
      "artifacts/text_processing\\Netclan20241159.txt\n",
      "artifacts/text_processing\\Netclan20241160.txt\n",
      "artifacts/text_processing\\Netclan20241161.txt\n",
      "artifacts/text_processing\\Netclan20241162.txt\n",
      "artifacts/text_processing\\Netclan20241163.txt\n",
      "147\n",
      "147\n",
      "147\n",
      "147\n",
      "147\n",
      "147\n",
      "147\n",
      "147\n",
      "147\n",
      "147\n",
      "147\n",
      "147\n",
      "147\n",
      "147\n",
      "147\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    text_analysis_config = config.get_text_analysis_config()\n",
    "    text_analysis = TextAnalysis(config=text_analysis_config)\n",
    "    text_analysis.text_analysis()\n",
    "    # text_processing.process_text()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "envTA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
